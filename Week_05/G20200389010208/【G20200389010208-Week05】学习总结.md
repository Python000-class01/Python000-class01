舆情监控：
Jieba - 分词、提取关键词
wordcloud - 词云展示结果
smtplib - 邮件通知
pymysql - 数据入库、ORM
语意情感分析

1. Jieba 分词与提取关键词     
（1）中文分词的难点：
A. 歧义切分
匹配模式的精确度
B: 未登录词的识别
未在词典和训练语料中出现
分词算法：
A. 常用词：基于词典、词库
B. 未登录词：基于词频统计分词，利用统计模型和决策算法做出最优切分

（2）jieba.cut
import jieba
jieba.enalbe_paddle()
cut_all = False 精确模式：做词云来展示
cut_all = True 全模式：切出所有高词频分词，提供给搜索引擎
use_paddle = True Paddle模式：分词速度比精确模式快
jieba.cut(‘keywords’) 不在词典中，但被基于词频的Viterbi算法识别，默认精确模式。
jieba.cut_for_search('keywords') 搜索引擎模式：尽可能划分分词，等待搜索引擎进一步处理。

（3）提取关键字（词）：输入整篇文本，提取和文档意义最相关的词。
A. 关键词分配：与给定词库对比，词库需要不断扩充
B. 关键词抽取：通过特定算法提取关键词
有监督学习方法：把关键词分成两类，一类画上标签，另一类进行验证，把新文章所有关键词抽取，对每一个关键词进行分类。
无监督学习方法：
import jieba
import jieba.analyse
a. 基于TF-IDF算法进行关键词抽取
b. 基于TestRank算法进行关键词抽取
去掉停止词：jieba.analyse.set_stop_words(stop_words)
自定义词典：jieba.load_userdict(user_dict)
eg. Python进阶训练营 3(权重) nt(词性：名词)
动态添加/删除词典：
jieba.add_word('需要添加的词')
jieba.del_word('需要删除的词')
HMM算法：自动识别不在词库里的词
jieba.cut(string2, HMM=False)
调整词频：错误识别
jieba.suggest_freq('需要合并的词', True)
jieba.suggest_freq(('分词1','分词2'), True)
支持多进程：实际需要重写算法
jieba.enable_parallel(4)
词典扩展：
=> github.com/fxsjy/jieba/tree/mster/extra_dict

（4）Jieba 优化
# 词频分析优化
# 移除低价值词语：
赞 0 //通过自定义词库设置低权重的方式
load_userdict() //加载自定义词典
# 句子的预处理：利用正则，移除中英文标点和数字
# 未登录词：利用HMM模型（隐马尔可夫模型）并结合Viterbi 算法
# TF-IDF算法：词频-逆文件频率，用来评估词语在文件集或语料中的重要程度。先在本地文件找词频高的（TF高），再到语料库里找词频低的（DF低），反推 IDF高，说明有很强的区分能力（TF*IDF）。

2. wordcloud 词云展示结果
pip3 install wordcloud
text_string = ‘,’.join(text_list) //转化字符串
wc = WordCloud(参数)
wc.generate_from_text(text_string)
中文乱码：
# wordcloud.py P30
# FONT_PATH = os.environ.get('FONT_PATH', os.path.join(FILE, 'PingFang.ttc')) //for MAC

3. smtplib 邮件发送
pip3 install email, smtplib
host_server =  'smtp.qq.com
smtp = SMTP_SSL(host_server)

4. pymysql 数据入库
pip3 install pymysql
一般流程：
START->创建connection->获取cursor->CRUD查询并获取数据->关闭cursor->关闭connection->END
conn = pymysql.connect{charset = 'utf8mb4'}
utf8: 只支持最长三个字节的 UTF-8 字符
utf8mb4: 支持4字节长度的 UTF-8 字符
utf-8: 在MySQL的命令模式中只能使用'utf8'，不能使用'utf-8'
/usr/local/mysql/bin/mysql -u root -p
mysql> show databases ;
mysql> create database test ;
mysql> use test ;
mysql> show tables ;
mysql> desc tb1 ;
mysql> select * from tb1 ;

sql 转成类
=>scrapydb
settgings.py
MYSQL_HOST = 'localhost'
class BlogscrapyPipeline(object):
class DBHelper():

=>sqlalchemy: orm 连接池
连接数据库
from sqlalchemy import  create_engine
添加数据
from sqlalchemy.orm import sessionmaker

5. 语意情感分析
A. 收集数据集：数据清洗
B. 设计文本的表示模型：数学表示，机器可读
C. 选择文本特征：提取关键词
D. 选择分类模型：决策树等

（1）snowNLP
pip install snownlp
A. 中文分词： s.words
B. 词性标注（马尔可夫模型）：list(s.tags)
C. 情感分析（朴素贝叶斯分类器）：s.sentiments
负向 0 <-----> 1 正向
D. 汉字转拼音（Trie树）：s.pinyin
E. 繁体转简体（繁简互转）：s.han
F. 提取关键字：s.keywords(limit=5)
G. 信息衡量：
s.tf   # 词频越大越重要
s.idf # 包含此条的文档越少，n越小，idf越大，说明词条t越重要
H. 重新训练
from snownlp import seg
seg.train('data.txt')
seg.save('seg.marshal')
# 修改 snownlp/seg/__init__.py 的 data_path 指向新的模型即可
# 训练模型
from snownlp import sentiment
sentiment.train('neg.txt','pos.txt')
sentiment.save('sentiment.marshal')
I. 特殊处理
变体字\中英混合\中文拼音\繁简\同音字\形近字\拆字\特殊字符

（2）wordvector 词向量
import numpy as np
标量：a = 1
向量：v1 =  np.array([1, 3])
矩阵：m1 = np.array([1, 0], [0, 1])
向量加减法：print(v1 + v2) print(v3 - v1)
曼哈顿距离：print(linalg.norm(v2 - v1, 1))
欧几里德距离：print(linalg.norm(v2 - v1, 2))
矩阵与标量运算：print(m1 + 1)
向量的维度（3维）：v3 = np.array([1, 1, 1]) print(v3.shape)
矩阵相加（形状相同）：print(m1.shape, m2.shape) print(m1 + m2)

（3）深度学习
import torch
import torchtext
from torchtext import vocab

（4）验证码识别

6. Flask Web 微框架
特点：灵活
dormousehole.readthedocs.io/en/latest
from flask import Flask, render_template
app = Flask(__name__)
flask run # app.py
if __name__ == '__main__':
     app.run(debug=True)
export FLASK_APP=p2_hello.py
flask run --host=0.0.0.0
默认单进程
MVC设计模式
MTV for flask
Jinja模板：{{text}}

A. 命名: 初始化脚本，代码的布局方法
https://docs.djangoproject.com/en/3.0/intro/tutorial01/
mysite/
    manage.py
    mysite/
        __init__.py
        settings.py
        urls.py
        asgi.py
        wsgi.py
﻿蓝图=>dormousehole.readthedocs.io/en/latest
B.﻿ Module
orm访问数据库 使用WTForms进行表单验证提交

C. Template 模板：渲染模板

D. View 视图：路由
# Werkzeug: localstack/localproxy
# 请求上下文：index和template通信
# Flask 信号
