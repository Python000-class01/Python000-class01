{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetURL = 'https://movie.douban.com/top250'\n",
    "agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContentFromURL(targetURL, agent, page_num):\n",
    "    res = []\n",
    "    urls = tuple(f'{targetURL}?start={ page * 25}' for page in range(page_num))\n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers={'user-agent': agent})\n",
    "        bs_info = bs(response.text, 'html.parser')\n",
    "        movie_titles, movie_urls = parse_movie_title(bs_info)\n",
    "        movie_ratings = parse_movie_ratings(bs_info)\n",
    "        comments_number = parse_movie_comment_num(bs_info)\n",
    "        movie_comments = parse_movie_comments_for_urls(movie_urls, headers={'user-agent': agent})\n",
    "        res.append([movie_titles, movie_ratings, comments_number, movie_comments])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_title(bs_info):\n",
    "    all_titles = []\n",
    "    movie_urls = []\n",
    "    for tags in bs_info.find_all('div', attrs={'class': 'hd'}):\n",
    "        for atag in tags.find_all('a'):\n",
    "            titles = []\n",
    "            movie_urls.append(atag.get('href'))\n",
    "            for stag in atag.find_all('span'):\n",
    "                titles.append(stag.get_text())\n",
    "            title = ''.join(titles)\n",
    "        all_titles.append(title)\n",
    "    return all_titles, movie_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_ratings(bs_info):\n",
    "    all_ratings = []\n",
    "    for rating_tag in bs_info.find_all('span', attrs={'class': 'rating_num'}):\n",
    "        all_ratings.append(float(rating_tag.get_text()))\n",
    "    return all_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_comment_num(bs_info):\n",
    "    all_comment_nums = []\n",
    "    for comment_tag in bs_info.find_all('div', attrs={'class': 'star'}):\n",
    "        comment_span = comment_tag.find('span', attrs={'class': None, 'property': None})\n",
    "        all_comment_nums.append(int(comment_span.get_text()[:-3]))\n",
    "    return all_comment_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_comments(movie_url, headers):\n",
    "    movie_response = requests.get(movie_url, headers=headers)\n",
    "    print(f'crawled movie comments {movie_response}')\n",
    "    movie_bs = bs(movie_response.text, 'html.parser')\n",
    "    short_comments = movie_bs.find_all('span', attrs={'class': 'short'})\n",
    "    short_comment = '\\nComment: '.join([''] + [i.get_text() for i in short_comments if len(i.contents) == 1])\n",
    "    time.sleep(1)\n",
    "    return short_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_comments_for_urls(movie_urls, headers):\n",
    "    comments = []\n",
    "    for movie_url in movie_urls:\n",
    "        comments.append(parse_movie_comments(movie_url, headers=headers))\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = getContentFromURL(targetURL, agent, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = []\n",
    "# for page in range(len(res)):\n",
    "#     for i in zip(res[page][0], res[page][1], res[page][2], res[page][3]):\n",
    "#         rows.append(list(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('C:\\\\Users\\\\hrajzl\\\\Desktop\\\\results.csv','w',encoding='utf-8-sig', newline='') as f:\n",
    "#     w = csv.writer(f)\n",
    "#     w.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import threading\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = {}\n",
    "movies_lock = threading.Lock() # For later when we want to update movies\n",
    "\n",
    "page_q = queue.Queue(25) # pages to crawl -- for movies list\n",
    "movie_q = queue.Queue(100) # movies to crawl -- for comments\n",
    "\n",
    "# 名称、评分、短评数量和前 5 条热门短评\n",
    "@dataclass\n",
    "class Movie:\n",
    "    title: str = None\n",
    "    rating: float = None\n",
    "    num_of_comments: int = None\n",
    "    comments: str = None\n",
    "    link: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_crawl():\n",
    "    crawling = True\n",
    "    while crawling:\n",
    "        try:\n",
    "            page_url = page_q.get(block=True, timeout=1)\n",
    "        except queue.Empty:\n",
    "            crawling = False\n",
    "            continue\n",
    "\n",
    "        response = requests.get(page_url, headers={'user-agent': agent})\n",
    "        print(f'crawled page {page_url}')\n",
    "        bs_info = bs(response.text, 'html.parser')\n",
    "\n",
    "        movie_titles, movie_links = parse_movie_title(bs_info)\n",
    "        movie_ratings = parse_movie_ratings(bs_info)\n",
    "        movie_comments_number = parse_movie_comment_num(bs_info)\n",
    "\n",
    "        for i in range(len(movie_titles)):\n",
    "            movie = Movie()\n",
    "            movie.title = movie_titles[i]\n",
    "            movie.rating = movie_ratings[i]\n",
    "            movie.num_of_comments = movie_comments_number[i]\n",
    "            movie.link = movie_links[i]\n",
    "            movie_q.put(movie)\n",
    "\n",
    "    print(\"Page all crawled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_crawl():\n",
    "    crawling = True\n",
    "    while crawling:\n",
    "        try:\n",
    "            movie = movie_q.get(block=True, timeout=1)\n",
    "        except queue.Empty:\n",
    "            crawling = False\n",
    "            continue\n",
    "        \n",
    "        movie_comments = parse_movie_comments(movie.link, headers={'user-agent': agent})\n",
    "        movie.comments = movie_comments\n",
    "        \n",
    "        movies_lock.acquire()\n",
    "        global movies\n",
    "        movies.update({\n",
    "            movie.title: movie\n",
    "        })\n",
    "        movies_lock.release()\n",
    "    print(\"Movie comments all crawled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_q_set_up(page_num):\n",
    "    for url in (f'https://movie.douban.com/top250?start={page * 25}' for page in range(page_num)):\n",
    "        page_q.put(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_run(page_threads_size, comments_threads_size):\n",
    "    page_threads = [ threading.Thread(target=page_crawl) for _ in range(page_threads_size) ]\n",
    "    comments_threads = [ threading.Thread(target=comments_crawl) for _ in range(comments_threads_size) ]\n",
    "    for t in page_threads:\n",
    "        t.start()\n",
    "    for t in page_threads:\n",
    "        t.join()\n",
    "    for t in comments_threads:\n",
    "        t.start()\n",
    "    for t in comments_threads:\n",
    "        t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_q_set_up(2) # Set up url queue to be crawled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawled page https://movie.douban.com/top250?start=25\n",
      "crawled page https://movie.douban.com/top250?start=0\n",
      "Page all crawled\n",
      "Page all crawled\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "crawled movie comments <Response [200]>\n",
      "Movie comments all crawled\n",
      "crawled movie comments <Response [200]>\n",
      "Movie comments all crawled\n",
      "Movie comments all crawled\n",
      "crawled movie comments <Response [200]>\n",
      "Movie comments all crawled\n",
      "Movie comments all crawled\n"
     ]
    }
   ],
   "source": [
    "movies = {}\n",
    "crawler_run(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "霸王别姬 / 再见，我的妾  /  Farewell My Concubine\n",
      "9.6\n",
      "1401605\n",
      "\n",
      "Comment: 不解,拍出过霸王别姬的人怎能拍出无极来\n",
      "Comment: 就凭这个，我愿意原谅陈凯歌一切的烂片 你只要伟大过一次就可以了 就凭这个 哥哥你是我心中永远不朽的传奇 你是全世界最大的角儿\n",
      "Comment: 城头变幻大王旗，一个《霸王别姬》，一个《活着》，道尽中国现当代史，百年内无可超越。\n",
      "Comment: 那么好的国粹，连日本人都知道要护着，你们说烧就烧……大多数开始于民国间的故事，最难捱的都是那段时间。\n",
      "Comment: 往事不要再提，人生已多风雨。他是霸王，你是虞姬，“我本是男儿郎，又不是女娇娥”，万丈红尘蹉跌走过半世纪。寥落繁华不由己，十万春花如梦里。剑还给你，命也还给你。“君王意气尽，贱妾何聊生？”陪你唱罢这出、我便离去...\n"
     ]
    }
   ],
   "source": [
    "print(movies['霸王别姬\\xa0/\\xa0再见，我的妾  /  Farewell My Concubine'].title)\n",
    "print(movies['霸王别姬\\xa0/\\xa0再见，我的妾  /  Farewell My Concubine'].rating)\n",
    "print(movies['霸王别姬\\xa0/\\xa0再见，我的妾  /  Farewell My Concubine'].num_of_comments)\n",
    "print(movies['霸王别姬\\xa0/\\xa0再见，我的妾  /  Farewell My Concubine'].comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\hrajzl\\\\Desktop\\\\results.csv','w',encoding='utf-8-sig', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    for k, v in movies.items():\n",
    "        title = movies[k].title\n",
    "        rating = movies[k].rating\n",
    "        num_of_comments = movies[k].num_of_comments\n",
    "        comments = movies[k].comments.strip()\n",
    "        w.writerow([title, rating, num_of_comments, comments])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantandfood-py3.7",
   "language": "python",
   "name": "plantandfood"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
